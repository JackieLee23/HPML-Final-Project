{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9yYTrgheDk64"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Subset\n",
        "import cv2\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.transforms import InterpolationMode\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "data_loc = \"drive/MyDrive/Columbia Research/Bose Einstein Condensation ML/data/rotating_gs\"\n",
        "model_loc = \"drive/MyDrive/Columbia Research/Bose Einstein Condensation ML/models\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "e9GCqMs4g-rX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "792e773e-979b-4c5c-d378-e95b487a1f4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = np.load(f\"{data_loc}/train.npz\")\n",
        "val_data = np.load(f\"{data_loc}/val.npz\")\n",
        "test_data = np.load(f\"{data_loc}/test.npz\")"
      ],
      "metadata": {
        "id": "XG-FLGuIMynX",
        "outputId": "97bb280b-5597-4065-cdca-a00709af2a90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'drive/MyDrive/Columbia Research/Bose Einstein Condensation ML/data/rotating_gs/train.npz'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4058895489.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{data_loc}/train.npz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{data_loc}/val.npz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{data_loc}/test.npz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/lib/_npyio_impl.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drive/MyDrive/Columbia Research/Bose Einstein Condensation ML/data/rotating_gs/train.npz'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14IT2zXJ1Hwc"
      },
      "source": [
        "# Visualize Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Size of train densities: {train_data['densities'].shape}\")\n",
        "print(f\"Size of val densities: {val_data['densities'].shape}\")\n",
        "print(f\"Size of test densities: {test_data['densities'].shape}\")\n",
        "print()\n",
        "print(f\"Size of train phase gradients: {train_data['phase_gradients'].shape}\")\n",
        "print(f\"Size of val phase gradients: {val_data['phase_gradients'].shape}\")\n",
        "print(f\"Size of test phase gradients: {test_data['phase_gradients'].shape}\")\n",
        "print()\n"
      ],
      "metadata": {
        "id": "S8Qj-TuSM5Z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize an example\n",
        "idx = 1\n",
        "fig, axes = plt.subplots(2, 2, figsize = (10, 10))\n",
        "axes[0, 0].imshow(train_data['densities'][idx], cmap='gray')\n",
        "axes[0, 1].imshow(train_data['phase_gradients'][idx, 0])\n",
        "axes[1, 0].imshow(train_data['phase_gradients'][idx, 1])\n",
        "axes[1, 1].imshow(1 - train_data['masks'][idx, 0], cmap = 'gray')\n",
        "\n",
        "axes[0, 0].set_title(\"Density\")\n",
        "axes[0, 1].set_title(\"x phase grad\")\n",
        "axes[1, 0].set_title(\"y phase grad\")\n",
        "axes[1, 1].set_title(\"Mask (valid region)\")\n",
        "\n",
        "plt.suptitle(\"Training Example\")"
      ],
      "metadata": {
        "id": "UpvYx1WnNnQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3UtcR5HmRJY"
      },
      "source": [
        "# Custom Datasets/Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9_XIfWiaX1S"
      },
      "outputs": [],
      "source": [
        "class BecDataset(Dataset):\n",
        "    def __init__(self, densities, phase_grads, masks, log_eps=1e-9):\n",
        "        \"\"\"\n",
        "        densities: (N, W, W)\n",
        "        phase_grads: (N, 2, W, W)\n",
        "        masks: (N, 2, W, W)\n",
        "        log_eps: added before log() for numerical stability\n",
        "        \"\"\"\n",
        "        self.densities = torch.tensor(densities).float().unsqueeze(1)\n",
        "        self.phase_grads = torch.tensor(phase_grads).float()\n",
        "        self.masks = torch.tensor(masks).float()\n",
        "        self.log_eps = log_eps\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.densities.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        density = self.densities[idx]\n",
        "        density = torch.log(density + self.log_eps)\n",
        "        phase_grad = self.phase_grads[idx]\n",
        "        mask = self.masks[idx]\n",
        "        return density, phase_grad, mask\n",
        "\n",
        "train_dataset = BecDataset(train_data['densities'], train_data['phase_gradients'], train_data['masks'])\n",
        "val_dataset = BecDataset(val_data['densities'], val_data['phase_gradients'], val_data['masks'])\n",
        "test_dataset = BecDataset(test_data['densities'], test_data['phase_gradients'], test_data['masks'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9_qgHHWna0S"
      },
      "outputs": [],
      "source": [
        "# Create corresponding dataloaders\n",
        "batch_size = 8\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers = 2)\n",
        "val_loader = DataLoader(val_dataset, num_workers = 2)\n",
        "test_loader = DataLoader(test_dataset, num_workers = 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoDKEJ72mjqg"
      },
      "source": [
        "# UNET Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NJHXlfQD8S8"
      },
      "outputs": [],
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(mid_channels, momentum = 0.2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels, momentum = 0.2),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upscaling then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True, add_skip = True):\n",
        "        super().__init__()\n",
        "\n",
        "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
        "        self.add_skip = add_skip\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
        "            self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        # input is CHW\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "        if self.add_skip:\n",
        "            x = torch.cat([x2, x1], dim=1)\n",
        "            #print(f\"dim after concatenation: {x.shape}\")\n",
        "        else:\n",
        "            x = x1\n",
        "        #print(f\"Running conv2d with {self.conv.double_conv[0].in_channels} in_channels, {self.conv.double_conv[-3].out_channels} out_channels\")\n",
        "        return self.conv(x)\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, channels, bilinear=True, add_skip = True, out_channels = 1, wrap_angles = False, verbose=False):\n",
        "        super(UNet, self).__init__()\n",
        "        self.channels = channels\n",
        "        self.bilinear = bilinear\n",
        "        self.verbose = verbose\n",
        "        self.wrap_angles = wrap_angles\n",
        "\n",
        "        self.inc = DoubleConv(channels[0], channels[1])\n",
        "        if add_skip:\n",
        "            #Repeat the final layer to add a skip connection in that layer\n",
        "            self.downs = nn.ModuleList([Down(channels[i], channels[i+1]) for i in range(1, len(channels)-1)] +\n",
        "                                       [Down(channels[-1], channels[-1])])\n",
        "            self.ups = nn.ModuleList([Up(2*channels[i+1], channels[i], bilinear, add_skip) for i in range(len(channels)-2, 0, -1)] +\n",
        "                                     [Up(2* channels[1], channels[1], bilinear, add_skip)])\n",
        "        else:\n",
        "            self.downs = nn.ModuleList([Down(channels[i], channels[i+1]) for i in range(1, len(channels)-1)])\n",
        "            self.ups = nn.ModuleList([Up(channels[i+1], channels[i], bilinear, add_skip) for i in range(len(channels)-2, 0, -1)])\n",
        "        self.outc = OutConv(channels[1], out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_stages = [self.inc(x)]\n",
        "        if self.verbose:\n",
        "            print(f\"x1 shape: {x_stages[-1].shape}\")\n",
        "\n",
        "        for down in self.downs:\n",
        "            x_stages.append(down(x_stages[-1]))\n",
        "            if self.verbose:\n",
        "                print(f\"Down stage shape: {x_stages[-1].shape}\")\n",
        "\n",
        "        x = x_stages.pop()\n",
        "        for up in self.ups:\n",
        "            x = up(x, x_stages.pop())\n",
        "            if self.verbose:\n",
        "                print(f\"Up stage shape: {x.shape}\")\n",
        "\n",
        "        x = self.outc(x)\n",
        "        if self.verbose:\n",
        "            print(f\"Output shape: {x.shape}\")\n",
        "\n",
        "        #Make between -pi and pi\n",
        "        if self.wrap_angles:\n",
        "            x = -F.relu(x + torch.pi) + 2*torch.pi\n",
        "            x = F.relu(x) - torch.pi\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cw_ar_kOOp9"
      },
      "outputs": [],
      "source": [
        "#Example usage, showing the channel sizes\n",
        "unet_8 = UNet(channels = [1, 8, 16, 32, 64, 128, 256, 512], out_channels = 2, verbose = True)\n",
        "dataiter = iter(train_loader)\n",
        "images, labels, masks = next(dataiter)\n",
        "print(images.shape)\n",
        "\n",
        "output = unet_8(images);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWO34qj1DujL"
      },
      "source": [
        "# Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "et6CeiDUNalj"
      },
      "outputs": [],
      "source": [
        "def predict(dataloader, model, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Return *torch tensors* for inputs, predictions, ground_truths, masks, and errors.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    inputs_list = []\n",
        "    predictions = []\n",
        "    ground_truths = []\n",
        "    masks = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets, mask_batch in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            mask_batch = mask_batch.to(device)\n",
        "            preds = model(inputs)\n",
        "\n",
        "            inputs_list.append(inputs)\n",
        "            predictions.append(preds)\n",
        "            ground_truths.append(targets)\n",
        "            masks.append(mask_batch)\n",
        "\n",
        "    # Concatenate tensors over batch dimension\n",
        "    return (\n",
        "        torch.cat(inputs_list, dim=0),\n",
        "        torch.cat(predictions, dim=0),\n",
        "        torch.cat(ground_truths, dim=0),\n",
        "        torch.cat(masks, dim=0),\n",
        "    )\n",
        "\n",
        "def train_model(train_loader, model, loss_fn, optimizer, epochs=10, device=\"cpu\"):\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for inputs, targets, masks in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_fn(outputs, targets, masks)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Run validation using torch-level predict()\n",
        "        _, predictions, ground_truths, masks = predict(\n",
        "            val_loader, model, device=device\n",
        "        )\n",
        "        val_loss = loss_fn(predictions, ground_truths, masks).item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1} Train Loss: {total_loss / len(train_loader)}, Val Loss: {val_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXQ-x50sYnG0"
      },
      "outputs": [],
      "source": [
        "#Custom Loss function\n",
        "#Ignores losses for masked out regions (mask = 1), as these are\n",
        "#Low density regions and we don't care whether we get them right or not\n",
        "\n",
        "def mse_with_mask(outputs, targets, masks, keep_first_dim = False):\n",
        "    if keep_first_dim:\n",
        "        return torch.mean(((outputs-targets)**2)*(1-masks), dim = (-1, -2, -3))\n",
        "    else:\n",
        "        return torch.mean(((outputs-targets)**2)*(1-masks))\n",
        "\n",
        "unet_8 = UNet(channels = [1, 8, 16, 32, 64, 128, 256, 512], out_channels = 2)\n",
        "optimizer = optim.Adam(unet_8.parameters(), lr=1e-3)\n",
        "\n",
        "train_model(train_loader, unet_8, mse_with_mask, optimizer, epochs=20, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxksg12vmoB6"
      },
      "source": [
        "# Test Set Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get test predictions\n",
        "inputs, predictions, truths, masks = predict(test_loader, unet_8, device = device)\n",
        "errors = mse_with_mask(predictions, truths, masks, keep_first_dim = True)\n",
        "\n",
        "print(f\"Error on the testing set: {errors.mean()}\")"
      ],
      "metadata": {
        "id": "r2l-vutZg5uX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to numpy arrays\n",
        "inputs = inputs.cpu().numpy()\n",
        "predictions = predictions.cpu().numpy()\n",
        "truths = truths.cpu().numpy()\n",
        "masks = masks.cpu().numpy()\n",
        "errors = errors.cpu().numpy()"
      ],
      "metadata": {
        "id": "VKqTkxBriifA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize a prediction\n",
        "idx = 1\n",
        "vmin = truths.min()\n",
        "vmax = truths.max()\n",
        "fig, axes = plt.subplots(2, 2, figsize = (10, 10))\n",
        "\n",
        "axes[0, 0].imshow(truths[idx, 0], vmin = vmin, vmax = vmax)\n",
        "axes[0, 1].imshow(predictions[idx, 0] * (1 - masks[idx, 0]), vmin = vmin, vmax = vmax)\n",
        "axes[1, 0].imshow(truths[idx, 1], vmin = vmin, vmax = vmax)\n",
        "axes[1, 1].imshow(predictions[idx, 1] * (1 - masks[idx, 0]), vmin = vmin, vmax = vmax)"
      ],
      "metadata": {
        "id": "Y3Rb-3rsOfB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Histogram of model test set errors and baseline errors.\n",
        "baseline_errors = np.var(truths, axis = (-1, -2, -3))\n",
        "all_errors = np.concatenate([errors, baseline_errors])\n",
        "\n",
        "bin_edges = np.histogram_bin_edges(all_errors, bins=40)\n",
        "plt.hist(errors, alpha = 0.5, bins = bin_edges, label = \"ML\", color='skyblue', edgecolor='black')\n",
        "plt.hist(baseline_errors, alpha = 0.5, bins = bin_edges, label = \"Baseline\", color='orange', edgecolor='black')\n",
        "plt.legend()\n",
        "plt.xlabel(\"Mean squared error on test example\")\n",
        "plt.ylabel(\"Frequency\")"
      ],
      "metadata": {
        "id": "lZXkwsDLjZuK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}